{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7a7e20-653b-4f62-8b19-3a739bef589d",
   "metadata": {},
   "source": [
    "Problem Statement\n",
    "---\n",
    "\n",
    "The objective is to extract meaningful topics using two different topic modelling approaches: LDA and BERTopic. Your task is to identify thematic structures within the movie synopses (use the synopsis column in the attached data) and compare the topics generated by the traditional method (LDA) with those produced by the more recent, embedding-based method (BERTopic).\n",
    "\n",
    "Instructions for Topic Modeling with LDA and BERTopic\n",
    "\n",
    "1. Data Preprocessing\n",
    "   - Clean the dataset by removing any special characters and numbers, perform tokenization, remove stop words, and apply stemming or lemmatization.\n",
    "   - Ensure the data is in a suitable format for each model.\n",
    "\n",
    "2. LDA Topic Modeling\n",
    "   - Convert the cleaned text data into a useful format.\n",
    "   - Train the LDA model, choosing an appropriate number of topics based on the dataset.\n",
    "   - Interpret and label the topics using the top words associated with each topic from the LDA model.\n",
    "   - Evaluate the LDA model performance using coherence scores and perplexity.\n",
    "\n",
    "3. BERTopic Modeling\n",
    "   - Install the BERTopic library if not already available in your environment.\n",
    "   - Fit the BERTopic model to the raw text data, which will utilize BERT embeddings for creating semantically rich topics.\n",
    "   - Interpret and label the topics using BERTopic's feature of extracting top words for each topic.\n",
    "   - Evaluate the BERTopic model by analyzing topic coherence and stability (coherence score).\n",
    "\n",
    "4. Comparison and Analysis\n",
    "   - Compare the topics generated by LDA and BERTopic, discussing the differences in terms of coherence, interpretability, and the granularity of topics.\n",
    "   - Analyze the practicality of both methods in terms of computational cost and ease of use.\n",
    "   - Discuss which method produced more meaningful and distinct topics and hypothesize why that might be the case.\n",
    "\n",
    "5. Conclusion\n",
    "   - Conclude with a reflection on the benefits and limitations of each method.\n",
    "   - Provide recommendations for which types of datasets each method might be better suited for.\n",
    "   - Encourage discussion about the potential for combining both methods or using them in different stages of a larger data analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2c539a44-b377-45ba-8a7d-aa4cb917baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import string \n",
    "\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "import ast\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "plt.style.use('rose-pine-moon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6e0f9338-0acc-4748-bd9d-e7d41b056332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Year</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Actors</th>\n",
       "      <th>Certificate</th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gen V</td>\n",
       "      <td>Action, Adventure, Comedy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13,679</td>\n",
       "      <td>2023–</td>\n",
       "      <td>From the world of \"The Boys\" comes \"Gen V,\" wh...</td>\n",
       "      <td>Jaz Sinclair, Chance Perdomo, Lizze Broadway, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://m.media-amazon.com/images/S/sash/4Fyxw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahsoka</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.8</td>\n",
       "      <td>69,947</td>\n",
       "      <td>2023–</td>\n",
       "      <td>After the fall of the Galactic Empire, former ...</td>\n",
       "      <td>Rosario Dawson, David Tennant, Natasha Liu Bor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://m.media-amazon.com/images/S/sash/4Fyxw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Loki</td>\n",
       "      <td>Action, Adventure, Fantasy</td>\n",
       "      <td>53 min</td>\n",
       "      <td>8.2</td>\n",
       "      <td>359,924</td>\n",
       "      <td>2021–</td>\n",
       "      <td>The mercurial villain Loki resumes his role as...</td>\n",
       "      <td>Tom Hiddleston, Owen Wilson, Sophia Di Martino...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://m.media-amazon.com/images/S/sash/4Fyxw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Wheel of Time</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.1</td>\n",
       "      <td>125,052</td>\n",
       "      <td>2021–</td>\n",
       "      <td>Set in a high fantasy world where magic exists...</td>\n",
       "      <td>Rosamund Pike, Daniel Henney, Madeleine Madden...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://m.media-amazon.com/images/S/sash/4Fyxw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One Piece</td>\n",
       "      <td>Action, Adventure, Comedy</td>\n",
       "      <td>60 min</td>\n",
       "      <td>8.4</td>\n",
       "      <td>109,063</td>\n",
       "      <td>2023–</td>\n",
       "      <td>In a seafaring world, a young pirate captain s...</td>\n",
       "      <td>Iñaki Godoy, Emily Rudd, Mackenyu, Vincent Regan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://m.media-amazon.com/images/S/sash/4Fyxw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Movie                       Genre Runtime  Rating    Votes  \\\n",
       "0              Gen V   Action, Adventure, Comedy     NaN     8.0   13,679   \n",
       "1             Ahsoka    Action, Adventure, Drama     NaN     7.8   69,947   \n",
       "2               Loki  Action, Adventure, Fantasy  53 min     8.2  359,924   \n",
       "3  The Wheel of Time    Action, Adventure, Drama  60 min     7.1  125,052   \n",
       "4          One Piece   Action, Adventure, Comedy  60 min     8.4  109,063   \n",
       "\n",
       "    Year                                           Synopsis  \\\n",
       "0  2023–  From the world of \"The Boys\" comes \"Gen V,\" wh...   \n",
       "1  2023–  After the fall of the Galactic Empire, former ...   \n",
       "2  2021–  The mercurial villain Loki resumes his role as...   \n",
       "3  2021–  Set in a high fantasy world where magic exists...   \n",
       "4  2023–  In a seafaring world, a young pirate captain s...   \n",
       "\n",
       "                                              Actors Certificate  \\\n",
       "0  Jaz Sinclair, Chance Perdomo, Lizze Broadway, ...         NaN   \n",
       "1  Rosario Dawson, David Tennant, Natasha Liu Bor...         NaN   \n",
       "2  Tom Hiddleston, Owen Wilson, Sophia Di Martino...         NaN   \n",
       "3  Rosamund Pike, Daniel Henney, Madeleine Madden...         NaN   \n",
       "4   Iñaki Godoy, Emily Rudd, Mackenyu, Vincent Regan         NaN   \n",
       "\n",
       "                                               Image  \n",
       "0  https://m.media-amazon.com/images/S/sash/4Fyxw...  \n",
       "1  https://m.media-amazon.com/images/S/sash/4Fyxw...  \n",
       "2  https://m.media-amazon.com/images/S/sash/4Fyxw...  \n",
       "3  https://m.media-amazon.com/images/S/sash/4Fyxw...  \n",
       "4  https://m.media-amazon.com/images/S/sash/4Fyxw...  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('movie_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "71271b2e-a068-4d0a-9f74-d6b881813a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Movie        500 non-null    object \n",
      " 1   Genre        500 non-null    object \n",
      " 2   Runtime      450 non-null    object \n",
      " 3   Rating       475 non-null    float64\n",
      " 4   Votes        500 non-null    object \n",
      " 5   Year         500 non-null    object \n",
      " 6   Synopsis     500 non-null    object \n",
      " 7   Actors       500 non-null    object \n",
      " 8   Certificate  8 non-null      object \n",
      " 9   Image        500 non-null    object \n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 39.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0c59d73b-5a2a-4094-b72a-e59ba270b798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>475.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.495158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.985591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Rating\n",
       "count  475.000000\n",
       "mean     7.495158\n",
       "std      0.985591\n",
       "min      1.600000\n",
       "25%      7.100000\n",
       "50%      7.600000\n",
       "75%      8.200000\n",
       "max      9.400000"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5a249ec3-1f0a-4bb2-9b4d-932b7a5fd132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Year</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Actors</th>\n",
       "      <th>Certificate</th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>450</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>483</td>\n",
       "      <td>56</td>\n",
       "      <td>75</td>\n",
       "      <td>475</td>\n",
       "      <td>234</td>\n",
       "      <td>499</td>\n",
       "      <td>500</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Battlestar Galactica</td>\n",
       "      <td>Animation, Action, Adventure</td>\n",
       "      <td>60 min</td>\n",
       "      <td>No votes yet</td>\n",
       "      <td>2023–</td>\n",
       "      <td>Plot kept under wraps.</td>\n",
       "      <td>Jaz Sinclair, Chance Perdomo, Lizze Broadway, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>https://m.media-amazon.com/images/S/sash/4Fyxw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>77</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Movie                         Genre Runtime  \\\n",
       "count                    500                           500     450   \n",
       "unique                   483                            56      75   \n",
       "top     Battlestar Galactica  Animation, Action, Adventure  60 min   \n",
       "freq                       3                           145      77   \n",
       "\n",
       "               Votes   Year                Synopsis  \\\n",
       "count            500    500                     500   \n",
       "unique           475    234                     499   \n",
       "top     No votes yet  2023–  Plot kept under wraps.   \n",
       "freq              25     50                       2   \n",
       "\n",
       "                                                   Actors Certificate  \\\n",
       "count                                                 500           8   \n",
       "unique                                                500           6   \n",
       "top     Jaz Sinclair, Chance Perdomo, Lizze Broadway, ...          15   \n",
       "freq                                                    1           2   \n",
       "\n",
       "                                                    Image  \n",
       "count                                                 500  \n",
       "unique                                                  1  \n",
       "top     https://m.media-amazon.com/images/S/sash/4Fyxw...  \n",
       "freq                                                  500  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include = 'O')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952e2e6-6fc1-48ff-98a5-19faf7619574",
   "metadata": {},
   "source": [
    "Defining Preprocessing Functions\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ce88c516-e176-4869-8b55-8aecb281c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(text):\n",
    "    \"\"\"\n",
    "    Function to preprocess text and return words as a comma-separated string\n",
    "    \"\"\"\n",
    "    # Step 1: Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Step 3: Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Step 4: Remove stopwords (common words like \"the,\" \"is,\" etc.)\n",
    "    custom_stopwords = stopwords.words('english')\n",
    "    \n",
    "    custom_stopwords.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', 'may', 'take',\n",
    "                             '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'come'\n",
    "                             'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', \n",
    "                             'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also'])\n",
    "\n",
    "    text = \" \".join([word for word in word_tokenize(text) if word.lower() not in custom_stopwords])\n",
    "\n",
    "    # Step 5: Remove short words (length < 3)\n",
    "    text = \" \".join([word for word in word_tokenize(text) if len(word) >= 3])\n",
    "\n",
    "    return text\n",
    "\n",
    " \n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Function for lemmatization\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize each word in the text\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(df, col, subset = 'all'):\n",
    "    \n",
    "    if 'preprocessed_text' not in df.columns: #<--- Check if 'preprocessed_text' column exists, if not, create it\n",
    "        df['preprocessed_text'] = \"\"\n",
    "\n",
    "    if subset != 'all':                  #<--- Select number of rows to perform the preprocessing on\n",
    "        subset_df = df.iloc[:subset].copy()\n",
    "    else:\n",
    "        subset_df = df.copy()\n",
    "\n",
    "    # Apply preprocessing to the subset of text data with progress bar\n",
    "    preprocessed_texts = []\n",
    "\n",
    "    for text in tqdm(subset_df[col]):\n",
    "        # Step 6: Preprocess the text\n",
    "        cleaned_text = cleanup_text(text) #<---invoking cleanup_text function to clean the text\n",
    "\n",
    "        # Step 7: Lemmatize the preprocessed text\n",
    "        lemmatized_text = lemmatize_text(cleaned_text) #<--- invoking the lemmatize_text function for lemmatization\n",
    "\n",
    "        # Append the processed text to the list\n",
    "        preprocessed_texts.append(lemmatized_text)\n",
    "\n",
    "    # # Convert the list of preprocessed texts to a comma-separated string\n",
    "    # preprocessed_texts_str = [text.replace(\" \", \",\") for text in preprocessed_texts]\n",
    "\n",
    "    # Store preprocessed words as comma-separated strings in the 'preprocessed_text' column\n",
    "    subset_df['preprocessed_text'] = preprocessed_texts\n",
    "    print(f\"{len(subset_df)} rows has been preprocessed\")\n",
    "    \n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73960b33-5b68-465c-ba1a-216b16346540",
   "metadata": {},
   "source": [
    "#### Data Preprocessing and data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e4213236-6088-4d8b-a4d7-27041c1cace3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e179ddb104b444d1874a36784b869009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 rows has been preprocessed\n"
     ]
    }
   ],
   "source": [
    "processed_ = preprocess_data(data,'Synopsis', subset ='all')\n",
    "\n",
    "text_list = processed_['preprocessed_text'].values.tolist()\n",
    "len(text_list)\n",
    "\n",
    "text_to_list = []\n",
    "for text in text_list:\n",
    "    words = text.split()\n",
    "    quoted_words = [\"'{}'\".format(word) for word in words]\n",
    "    text_to_list.append(\"[{}]\".format(', '.join(quoted_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ed814078-b3a1-4235-bc33-8d4861d7c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_of_lists = [ast.literal_eval(text) for text in text_to_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88780e-fd9d-4507-8373-5ffe9f3b1083",
   "metadata": {},
   "source": [
    "Now that we have prprocessed our data, lets try to implement BoW (bag of words) to get a dictionary that would represent words and the number of times appeared. and corpus which would be used in bulding our LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b61ded-869e-4c11-9746-db6103e67dcb",
   "metadata": {},
   "source": [
    "#### BoW Dictionary & Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "41a5edc2-c860-421b-8d1b-b1f5e6c60a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(word_lists)  # Dictionary of words (key) & number of times each word appeard (value)\n",
    "\n",
    "dictionary.filter_extremes(no_below=5)   # filter out words that appeared inless than 5 sentences\n",
    "\n",
    "# Convert the preprocessed word lists to bag_of_words representation\n",
    "corpus = [dictionary.doc2bow(text) for text in word_list_of_lists]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29580b4c-65ad-4e14-9e1e-d354c710baf3",
   "metadata": {},
   "source": [
    "#### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "66b77e6e-0dd3-49fa-a905-8746555ebdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                      id2word=dictionary,\n",
    "                                      num_topics=10,   \n",
    "                                      random_state=100,\n",
    "                                      chunksize=100,\n",
    "                                      passes=10,\n",
    "                                      per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "55fc99cb-2e56-4829-a10e-9e0d7421bf0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: mission | crew | galaxy | mystery | young | must | year | land | return | secret\n",
      "Topic 1: two | day | team | boy | fight | evil | batman | crime | must | century\n",
      "Topic 2: adventure | friend | new | life | one | series | way | game | year | love\n",
      "Topic 3: series | earth | world | planet | adventure | find | home | one | time | war\n",
      "Topic 4: world | dragon | human | find | life | follows | power | set | become | journey\n",
      "Topic 5: epic | american | world | child | discovers | family | power | young | must | two\n",
      "Topic 6: adventure | city | young | ninja | superheroes | new | art | martial | detective | skill\n",
      "Topic 7: world | save | based | young | evil | group | battle | ancient | known | partner\n",
      "Topic 8: band | school | named | high | find | help | group | star | girl | different\n",
      "Topic 9: new | city | fight | life | power | mysterious | find | family | island | help\n"
     ]
    }
   ],
   "source": [
    "# Get the topic-word probabilities\n",
    "topic_word_probs = lda_model.get_topics()\n",
    "\n",
    "# Get the vocabulary from the id2word dictionary\n",
    "vocab = list(dictionary.values())\n",
    "\n",
    "n_top_words = 10\n",
    "for i, topic_probs in enumerate(topic_word_probs):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_probs)][:-(n_top_words + 1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc623b-1ddd-47a6-b41a-8c04c325a6b6",
   "metadata": {},
   "source": [
    "Based on the topics, here are the possible context.\n",
    "\n",
    "- Topic 0: Galactic Mission\n",
    "  \n",
    "Keywords: mission, crew, galaxy, mystery, young, must, year, land, return, secret\n",
    "Possible Interpretation: This topic seems to revolve around a space mission or adventure involving a crew exploring a galaxy, with elements of mystery and secrecy\n",
    "\n",
    "- Topic 1: Dynamic Team and Crime Fighting\n",
    "\n",
    "Keywords: two, day, team, boy, fight, evil, batman, crime, must, century\n",
    "Possible Interpretation: This topic suggests a storyline involving a dynamic team, possibly led by a young boy, engaged in fighting crime and facing challenges across different time periods.\n",
    "\n",
    "- Topic 2: Life's Adventure and Friendship\n",
    "\n",
    "Keywords: adventure, friend, new, life, one, series, way, game, year, love\n",
    "Possible Interpretation: This topic is about the adventures of a character or group of friends in a new phase of life, possibly exploring love and relationships.\n",
    "\n",
    "- Topic 3: Time Travel and War\n",
    "\n",
    "Keywords: series, earth, world, planet, adventure, find, home, one, time, war\n",
    "Possible Interpretation: This topic suggests a storyline involving time travel, war, and the search for a home or a resolution in different worlds or planets.\n",
    "\n",
    "- Topic 4: Fantasy World with Dragons and Power\n",
    "\n",
    "Keywords: world, dragon, human, find, life, follows, power, set, become, journey\n",
    "Possible Interpretation: This topic involves a fantasy world with dragons, humans, and a journey where characters seek power and transformation.\n",
    "\n",
    "- Topic 5: Epic American Family Adventure\n",
    "\n",
    "Keywords: epic, American, world, child, discovers, family, power, young, must, two\n",
    "Possible Interpretation: This topic may depict an epic adventure involving an American family, where a child discovers power and must navigate challenges with another character.\n",
    "\n",
    "- Topic 6: Adventure in the City with Superheroes\n",
    "\n",
    "Keywords: adventure, city, young, ninja, superheroes, new, art, martial, detective, skill\n",
    "Possible Interpretation: This topic suggests an adventure set in a city involving young characters, ninjas, superheroes, martial arts, and detective skills.\n",
    "\n",
    "- Topic 7: World-saving Battle with Evil\n",
    "\n",
    "Keywords: world, save, based, young, evil, group, battle, ancient, known, partner\n",
    "Possible Interpretation: This topic revolves around a world-saving mission where young characters, possibly part of a group, engage in a battle against ancient evil forces.\n",
    "\n",
    "- Topic 8: High School Band and Star Adventures\n",
    "\n",
    "Keywords: band, school, named, high, find, help, group, star, girl, different\n",
    "Possible Interpretation: This topic involves a narrative centered around a high school band, possibly named or unique, where characters find help and embark on adventures related to stars.\n",
    "\n",
    "- Topic 9: Mysterious City Life\n",
    "\n",
    "Keywords: new, city, fight, life, power, mysterious, find, family, island, help\n",
    "Possible Interpretation: This topic suggests a storyline set in a new city where characters engage in fights, explore life, possess mysterious powers, and seek the support of family or help on an island."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d788e-d2cf-4945-9e59-068c7b3f6406",
   "metadata": {},
   "source": [
    "#### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "39c0ce77-29ff-4307-9973-a61b0169c88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  72.17637793418177\n"
     ]
    }
   ],
   "source": [
    "log_perplexity = lda_model.log_perplexity(corpus)\n",
    "\n",
    "# Convert log perplexity to perplexity\n",
    "perplexity = 2**(-log_perplexity)\n",
    "print('Perplexity: ', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "df496227-a977-4429-8971-6989df3e9c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.3861074002785128\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=word_list_of_lists, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a3b7b-8f02-4c4c-af97-32f8f873f45d",
   "metadata": {},
   "source": [
    "The LDA model's perplexity of 72 suggests that, on average, the model assigns a lower probability to the observed data. This score is reasonable. The  coherence score of 39 indicates a moderate level of semantic similarity among high-scoring words within topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7923d9e-5450-445e-bda4-27bcdcd44e2e",
   "metadata": {},
   "source": [
    "#### Creating a bertopicmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "332e8617-6bc1-486d-80ef-4ec0fc39ad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 22:12:50,047 - BERTopic - Transformed documents to Embeddings\n",
      "2023-11-27 22:12:54,752 - BERTopic - Reduced dimensionality\n",
      "2023-11-27 22:12:54,776 - BERTopic - Clustered reduced embeddings\n",
      "2023-11-27 22:12:54,832 - BERTopic - Reduced number of topics from 8 to 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['adventure', 'world', 'life', 'must', 'series', 'monster', 'find', 'story', 'group', 'island']\n",
      "Topic 1: ['agent', 'spy', 'international', 'secret', 'discovers', 'operative', 'become', 'secretly', 'cia', 'job']\n",
      "Topic 2: ['superheroes', 'crime', 'city', 'superhero', 'team', 'fight', 'justice', 'adventure', 'hero', 'comic']\n",
      "Topic 3: ['time', 'travel', 'present', 'world', 'around', 'space', 'people', 'traveler', 'life', 'day']\n",
      "Topic 4: ['crew', 'galaxy', 'year', 'earth', 'planet', 'alien', 'us', 'space', 'mission', 'captain']\n",
      "Topic 5: ['friend', 'adventure', 'family', 'girl', 'way', 'town', 'best', 'boy', 'series', 'group']\n",
      "Topic 6: ['magic', 'demon', 'world', 'peace', 'boy', 'school', 'magician', 'ichigo', 'destiny', 'soul']\n",
      "Topic 7: ['world', 'ninja', 'dragon', 'japan', 'quest', 'force', 'power', 'princess', 'great', 'warrior']\n"
     ]
    }
   ],
   "source": [
    "documents = processed_[\"preprocessed_text\"].tolist()\n",
    "\n",
    "# Create a document-term matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "documents_as_strings = [\" \".join(vectorizer.get_feature_names_out()[doc.indices]) for doc in X]\n",
    "\n",
    "# Create and fit the BERTopic model\n",
    "bertopic_model = BERTopic(nr_topics=10)\n",
    "topics, _ = bertopic_model.fit_transform(documents_as_strings)\n",
    "\n",
    "# Get the topics and top words for each topic\n",
    "top_words = bertopic_model.get_topic_info()\n",
    "\n",
    "# Print the topics and top words\n",
    "for i, (topic, words) in enumerate(zip(topics, top_words[\"Representation\"])):\n",
    "    print(f\"Topic {i}: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14cc8bc-cdd9-49a0-863c-af0238d65e18",
   "metadata": {},
   "source": [
    "- Topic 0: Adventure World:\n",
    "\n",
    "Keywords: world, life, adventure, monster, group, dangerous, island.\n",
    "Possible Interpretation: This topic may represent movies involving adventurous journeys in a fantasy world with encounters of monsters and dangerous situations.\n",
    "\n",
    "- Topic 1: International Spy Elite:\n",
    "\n",
    "Keywords: agent, international, spy, secret, discovers, operative, elite, secretly, CIA.\n",
    "Possible Interpretation: This topic suggests espionage and international intrigue, featuring secret agents, discoveries, and elite operations, possibly with a focus on the CIA.\n",
    "\n",
    "- Topic 2: Time Travel Exploration:\n",
    "\n",
    "Keywords: time, travel, present, world, space, traveler, day.\n",
    "Possible Interpretation: This topic could indicate movies centered around time travel, exploring different time periods and the impact of time travel on the present world.\n",
    "\n",
    "- Topic 3: Galactic Mission:\n",
    "\n",
    "Keywords: crew, galaxy, planet, year, earth, mission, space, captain.\n",
    "Possible Interpretation: This topic may represent space exploration and missions involving a crew, galaxy, and planets, led by a captain.\n",
    "\n",
    "- Topic 4: Family Adventure Series:\n",
    "\n",
    "Keywords: friend, adventure, family, series, girl, boy, town, best, new.\n",
    "Possible Interpretation: This topic could encompass family-oriented adventure series, possibly involving friends, new experiences, and life in a town.\n",
    "\n",
    "- Topic 5: Superhero Comic Justice:\n",
    "\n",
    "Keywords: superheroes, city, crime, superhero, team, hero, fight, justice, adventure, comic.\n",
    "Possible Interpretation: This topic suggests superhero-themed movies with a focus on combating crime, teamwork, and justice, potentially inspired by comic book stories.\n",
    "\n",
    "- Topic 6: Ninja Samurai World:\n",
    "\n",
    "Keywords: ninja, Japan, skill, samurai, Uzumaki, Naruto, world, leader, great, Hokage.\n",
    "Possible Interpretation: This topic may represent movies set in a world of ninjas and samurais, with a notable reference to Naruto, a well-known ninja character.\n",
    "\n",
    "- Topic 7: Magical Demon Land:\n",
    "\n",
    "Keywords: world, demon, magic, princess, peace, king, ancient, friend, young, land.\n",
    "Possible Interpretation: This topic could involve a magical world with demons, princesses, and a quest for peace, featuring ancient lands and friendships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4ffa3e5f-5cea-4034-bc50-344d67bda28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=model, texts=word_list_of_lists, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2746e-6374-40a9-8afa-33ba77bf2d92",
   "metadata": {},
   "source": [
    "Coherence scores closer to 1.0 suggest that the topics are well-separated and the words within each topic are highly associated. But here wehave 1.0 which means the model is perfectly separating words and this sort of suggests overfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a132b5d6-5373-4271-8580-4d4555995817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tuning\n",
    "# documents = processed_[\"preprocessed_text\"].tolist()\n",
    "\n",
    "# # Create a document-term matrix using CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# documents_as_strings = [\" \".join(vectorizer.get_feature_names_out()[doc.indices]) for doc in X]\n",
    "\n",
    "# # Create and fit the BERTopic model\n",
    "# bertopic_model = BERTopic(\n",
    "#     nr_topics=10,\n",
    "#     top_n_words=10,   # Number of top words per topic\n",
    "#     # nr_top_terms=15,  # Number of top terms to consider when calculating topic representation\n",
    "#     n_gram_range=(1, 1),  # Adjust the n-gram range\n",
    "#     min_topic_size=5,  # Minimum number of documents in a topic\n",
    "#     # umap_args={'n_neighbors': 15, 'n_components': 5, 'metric': 'cosine'},  # Adjust UMAP parameters\n",
    "#     # hdbscan_args={'min_cluster_size': 10, 'metric': 'euclidean'}  # Adjust HDBSCAN parameters\n",
    "# )\n",
    "# topics, _ = bertopic_model.fit_transform(documents_as_strings)\n",
    "\n",
    "# # Get the topics and top words for each topic\n",
    "# top_words = bertopic_model.get_topic_info()\n",
    "\n",
    "# # Print the topics and top words\n",
    "# for i, (topic, words) in enumerate(zip(topics, top_words[\"Representation\"])):\n",
    "#     print(f\"Topic {i}: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642505d-783a-4c5a-8f87-1f8c1b34cc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c4e6645-37b1-4cc8-a7cc-07432480de97",
   "metadata": {},
   "source": [
    "- Coherence:\n",
    "The coherence scores for the LDA model indicate a perplexity of 72.77 and a coherence score of 39. While perplexity is relatively high, the coherence score suggests that the topics generated by LDA might have some overlapping terms, impacting the overall interpretability. On the other hand, BERTopic consistently produces a coherence score of 1.0, indicating non-overlapping and well-defined topics. This suggests that BERTopic excels in generating topics that are distinct and easily interpretable.\n",
    "\n",
    "- Interpretability:\n",
    "LDA assigns probabilities to each word in the vocabulary for its association with a particular topic. Topics are then represented by the most probable words. While this provides a certain level of interpretability, the coherence score suggests that there might be some ambiguity in the topics. In contrast, BERTopic represents topics by extracting the most representative words in the cluster. This approach potentially captures more contextually relevant words, enhancing the overall interpretability of topics.\n",
    "\n",
    "- Granularity of Topics:\n",
    "The granularity of topics refers to how specific and detailed the identified themes are. LDA, with a chosen number of 10 topics, tends to generate broader themes. The interpretability of these topics may be affected by a certain level of generality. BERTopic, leveraging BERT embeddings, has the potential to capture more nuanced and specific topics due to its contextual understanding. The high coherence score further supports the notion that BERTopic produces well-separated and granular topics.\n",
    "\n",
    "In summary, the coherence, interpretability, and granularity of topics suggest that BERTopic outperforms LDA in this particular context. The consistently high coherence score indicates that the topics generated by BERTopic are not only more interpretable but also more distinct and granular, providing a richer representation of the underlying thematic structures within the movie synopses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5264218-fafa-4d91-9c8b-7fbb6ddb4580",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d21140-cece-4d58-8e74-d0e9a5f030d8",
   "metadata": {},
   "source": [
    "- LDA:\n",
    "Latent Dirichlet Allocation (LDA) is a robust and computationally efficient method for topic modeling. It is particularly well-suited for large datasets, providing probabilistic insights into word-topic associations. However, its reliance on bag-of-words representations and the assumption of a static topic distribution may limit its effectiveness in capturing dynamic or nuanced semantic relationships.\n",
    "\n",
    "- BERTopic:\n",
    "BERTopic, leveraging BERT embeddings, excels in capturing context and semantic nuances. It offers highly coherent and interpretable topics, even with smaller datasets. While computationally more demanding and requiring a pre-trained BERT model, BERTopic's ability to understand word context enhances its performance in complex thematic structures.\n",
    "\n",
    "- Recommendations:\n",
    "For larger datasets where computational efficiency is crucial, LDA is a solid choice. It performs well when a static topic distribution assumption is reasonable. On the other hand, BERTopic is recommended for smaller datasets where capturing context and semantic relationships is a priority, offering enhanced interpretability and coherence.\n",
    "\n",
    "Combining Both Methods:\n",
    "A strategic combination of LDA and BERTopic in different stages of analysis can leverage their respective strengths. LDA can provide an initial broad overview of topics, and BERTopic can refine and enhance the granularity of identified themes. This integrated approach allows for a comprehensive and nuanced understanding of complex datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225f054-1322-47a8-8e87-93044d2a53ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
